{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM6hHgJJ5CJ8"
   },
   "source": [
    "# Define your ouwn data generator    \n",
    "## It choses a batch of images from the training data folder\n",
    "## It applies augmentation on each of the images\n",
    "## It crops a random patch from each of the augmented data, following a certain pdf distribution (if given)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33845,
     "status": "ok",
     "timestamp": 1585652060406,
     "user": {
      "displayName": "ESTIBALIZ GOMEZ DE MARISCAL",
      "photoUrl": "",
      "userId": "04592796515262324641"
     },
     "user_tz": -120
    },
    "id": "36Qiexn9ZOnR",
    "outputId": "99f288a4-b9b6-4819-b739-1cbc510eb229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77838,
     "status": "ok",
     "timestamp": 1585652121089,
     "user": {
      "displayName": "ESTIBALIZ GOMEZ DE MARISCAL",
      "photoUrl": "",
      "userId": "04592796515262324641"
     },
     "user_tz": -120
    },
    "id": "Ei0lGRuh-naB",
    "outputId": "933869a5-4f05-4f66-b964-d3c6bac1ef8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
      "\u001b[K     |████████████████████████████████| 86.3MB 50kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 57.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 54.1MB/s \n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (46.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=ff0ec4ceebfc5ab7012ee30f8d8839ec248a2b30832b2cdcb98a4aa5734614a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built gast\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
      "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
      "  Found existing installation: tensorboard 2.1.1\n",
      "    Uninstalling tensorboard-2.1.1:\n",
      "      Successfully uninstalled tensorboard-2.1.1\n",
      "  Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Found existing installation: tensorflow 2.2.0rc1\n",
      "    Uninstalling tensorflow-2.2.0rc1:\n",
      "      Successfully uninstalled tensorflow-2.2.0rc1\n",
      "Successfully installed gast-0.2.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n",
      "Collecting SimpleITK\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d8/53338c34f71020725ffb3557846c80af96c29c03bc883551a2565aa68a7c/SimpleITK-1.2.4-cp36-cp36m-manylinux1_x86_64.whl (42.5MB)\n",
      "\u001b[K     |████████████████████████████████| 42.5MB 68kB/s \n",
      "\u001b[?25hInstalling collected packages: SimpleITK\n",
      "Successfully installed SimpleITK-1.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0\n",
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGrrshbxZT-B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/MU_Lux_CZ\")\n",
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roP49Ysz5CKC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.morphology\n",
    "import cv2\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# print(keras.__version__)\n",
    "# Follow: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "## Define random crop function\n",
    "def random_crop(x, y,y_marks,weights, random_crop_size_input, random_crop_size_output, pdf=1, sync_seed=None):\n",
    "    \"\"\"\n",
    "    x is 2D data\n",
    "    pdf is a 2D image representing the sampling distribution to crop the patches\n",
    "    \"\"\"    \n",
    "    np.random.seed(sync_seed)\n",
    "    w, h = x.shape[1], x.shape[0]\n",
    "#     rangew = (w - random_crop_size[0])\n",
    "#     rangeh = (h - random_crop_size[1])\n",
    "    rangew = (w - np.floor(random_crop_size_input[0]//2))\n",
    "    rangeh = (h - np.floor(random_crop_size_input[1]//2))\n",
    "#     offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "#     offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    if pdf == 1:\n",
    "        offsetw = np.random.randint(np.floor(random_crop_size_input[0]//2), rangew)\n",
    "        offseth = np.random.randint(np.floor(random_crop_size_input[1]//2), rangeh)\n",
    "    else:\n",
    "        l = np.int(np.floor(random_crop_size_input[0]//2))\n",
    "        u = np.int(np.floor(random_crop_size_input[1]//2))\n",
    "        pdf_im = np.ones(y.shape)\n",
    "        pdf_im[y>0]=pdf\n",
    "        pdf_im = pdf_im[l:-l,u:-u]\n",
    "        prob = np.float32(pdf_im)\n",
    "        prob = prob.ravel()/np.sum(prob)\n",
    "        choices = np.prod(pdf_im.shape)\n",
    "        index = np.random.choice(choices, size=1,p = prob)\n",
    "        coordinates = np.unravel_index(index, shape=pdf_im.shape)\n",
    "        offsetw = coordinates[0][0] # pdf first coordinate corresponds to the \"x\" axis = width\n",
    "        offsetw = offsetw + l\n",
    "        offseth = coordinates[1][0] # pdf second coordinate corresponds to the \"y\" axis = height\n",
    "        offseth = offseth + u\n",
    "    \n",
    "    lr = offsetw-np.floor(random_crop_size_input[0]//2)\n",
    "    lr = lr.astype(np.int)\n",
    "    ur = offsetw+np.round(random_crop_size_input[0]//2)\n",
    "    ur = ur.astype(np.int)\n",
    "    \n",
    "    lc = offseth-np.floor(random_crop_size_input[1]//2)\n",
    "    lc = lc.astype(np.int)\n",
    "    uc = offseth+np.round(random_crop_size_input[1]//2)\n",
    "    uc = uc.astype(np.int)\n",
    "    \n",
    "    x_patch = x[lr:ur,lc:uc]\n",
    "    \n",
    "    y_patch = y[lr:ur,lc:uc]\n",
    "    y_marks_patch = y_marks[lr:ur,lc:uc]\n",
    "    weights_patch = weights[lr:ur,lc:uc]\n",
    "    if random_crop_size_input != random_crop_size_output:\n",
    "      y_patch=center_crop(y_patch, random_crop_size_output)\n",
    "      y_marks_patch=center_crop(y_marks_patch, random_crop_size_output)\n",
    "      weights_patch=center_crop(weights_patch, random_crop_size_output)\n",
    "\n",
    "#     lr = offsetw-np.floor(random_crop_size_output[0]//2)\n",
    "#     lr = lr.astype(np.int)\n",
    "#     ur = offsetw+np.round(random_crop_size_output[0]//2)\n",
    "#     ur = ur.astype(np.int)\n",
    "    \n",
    "#     lc = offseth-np.floor(random_crop_size_output[1]//2)\n",
    "#     lc = lc.astype(np.int)\n",
    "#     uc = offseth+np.round(random_crop_size_output[1]//2)\n",
    "#     uc = uc.astype(np.int)    \n",
    "    return x_patch, y_patch, y_marks_patch, weights_patch\n",
    "    \n",
    "def center_crop(x, crop_size):\n",
    "    \"\"\"\n",
    "    Crop function when there is no padding in the the CNN and we need to reduce the ground truth. \n",
    "    x is 3D data but patches are only cropped in 2D. x dimensions (z,y,x)\n",
    "    \"\"\"    \n",
    "    halfh = (x.shape[0]-crop_size[0])//2\n",
    "    halfw = (x.shape[1]-crop_size[1])//2\n",
    "\n",
    "    return x[halfh:-halfh,halfw:-halfw] \n",
    "\n",
    "## Define data augmentation for the case in which x has dimensions (t,i,j) and y (1,i,j). \n",
    "# The output of this function are augmented_x with dimensions (1,t,i,j,c) and augmented_y with (1,1,i,j,c)\n",
    "\n",
    "def data_augmentation_weightedmaps(x, y, y_marks, y_weights, self, seed = None):\n",
    "      \n",
    "    data_gen_args = dict(rotation_range=self.rotation_range,\n",
    "                            width_shift_range=self.width_shift_range,\n",
    "                            height_shift_range=self.height_shift_range,\n",
    "                            shear_range=self.shear_range,\n",
    "                            zoom_range=self.zoom_range,\n",
    "                            horizontal_flip=self.horizontal_flip,\n",
    "                            fill_mode=self.fill_mode) \n",
    "\n",
    "    # Initialize Keras data augmentaiton\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    weight_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mark_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    # Apply the same data augmentation to all the images, corresponding mask, \n",
    "    # marks, weights and pdf for cropping\n",
    "    # self.dim_input=(220,220)\n",
    "\n",
    "    input_channel = np.zeros((1,self.dim_input[0], self.dim_input[1]))\n",
    "    weights_channel = np.zeros((1,self.dim_output[0], self.dim_output[1]))\n",
    "    output_channel = np.zeros((1,self.dim_output[0], self.dim_output[1]))\n",
    "    marks_channel = np.zeros((1,self.dim_output[0], self.dim_output[1]))\n",
    "    \n",
    "    \n",
    "    # Generate the same seed:\n",
    "    seed = np.random.randint(10000000)       \n",
    "    # if self.module == \"train\":\n",
    "    augmented_x = np.zeros((x.shape[0], x.shape[1]))\n",
    "#             augmented_y = np.zeros((y.shape[0], y.shape[1], y.shape[2]))\n",
    "    \n",
    "    # self_pdf = self.pdf\n",
    "    \n",
    "    # if self_pdf == None:\n",
    "    #     self_pdf = np.ones((x.shape[0], x.shape[1]))\n",
    "    # augmented_pdf = np.zeros((1,self_pdf.shape[0], self_pdf.shape[1], 1))\n",
    "    \n",
    "    x = x.reshape((1, x.shape[0], x.shape[1], 1))\n",
    "    for batch in image_datagen.flow(x,  batch_size=1, seed = seed):\n",
    "        augmented_x = batch[0,:,:,0]\n",
    "        break\n",
    "    # Apply the same data augmentation to the ground truth. \n",
    "    # GROUND TRUTH IS ASSUMED TO BE 2D\n",
    "    # self.dim_output=(36,36)\n",
    "    # output_channel.dimensions = (1,36,36)\n",
    "    aux_y = np.copy(y)\n",
    "    aux_y = aux_y.reshape((1, y.shape[0], y.shape[1], 1))\n",
    "    for batch in mask_datagen.flow(aux_y,  batch_size=1, seed = seed):\n",
    "        augmented_y = batch[0,:,:,0]\n",
    "        break\n",
    "\n",
    "    aux_y_marks = np.copy(y_marks)\n",
    "    aux_y_marks = aux_y_marks.reshape((1, y_marks.shape[0], y_marks.shape[1], 1))\n",
    "    for batch in mask_datagen.flow(aux_y_marks, batch_size=1, seed = seed):\n",
    "        augmented_y_marks = batch[0,:,:,0]\n",
    "        break\n",
    "\n",
    "    aux_weights = np.copy(y_weights)\n",
    "    aux_weights = aux_weights.reshape((1, y_weights.shape[0], y_weights.shape[1], 1))\n",
    "    for batch in mask_datagen.flow(aux_weights,  batch_size=1, seed = seed):\n",
    "        augmented_weights = batch[0,:,:,0]\n",
    "        break\n",
    "\n",
    "    # aux_pdf = np.copy(self_pdf)\n",
    "    # aux_pdf = aux_pdf.reshape((1, self_pdf.shape[0], self_pdf.shape[1],1))\n",
    "    # for batch in mask_datagen.flow(aux_pdf,  batch_size=1, seed = seed):\n",
    "    #     augmented_pdf = batch\n",
    "    #     break\n",
    "            \n",
    "    augmented_y[augmented_y>0]=1\n",
    "    augmented_y_marks[augmented_y_marks>0]=1\n",
    "    \n",
    "    random_crop_size_input = (self.dim_input[0],  self.dim_input[1])\n",
    "    random_crop_size_output = (self.dim_output[0],  self.dim_output[1])\n",
    "\n",
    "    input_im,output_labels,output_marks,weights = random_crop(augmented_x,augmented_y,augmented_y_marks,augmented_weights, random_crop_size_input, random_crop_size_output, pdf=self.pdf, sync_seed=seed)\n",
    "\n",
    "    return input_im, output_labels, output_marks, weights\n",
    "\n",
    "\n",
    "# Follow: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "## Define DataGenerator \n",
    "def median_normalization(image):\n",
    "    '''\n",
    "    Normalize the values in the image to have median 0.5\n",
    "    '''\n",
    "    # image_ = image / 255 + (.5 - np.median(image / 255)) # Original from MU-Lux_CZ\n",
    "    # The function assumes that values are in the [0,1] range\n",
    "    image_ = image + (.5 - np.median(image))\n",
    "    return np.maximum(np.minimum(image_, 1.), .0)\n",
    "\n",
    "def hist_equalization(image):\n",
    "    \"\"\"\n",
    "    Contrast limited adaptive histogram equalization for normalization fo values\n",
    "    \"\"\"\n",
    "    # return cv2.equalizeHist(image) / 255 # Original from MU-Lux_CZ\n",
    "    # The function assumes that values are in the [0,1] range\n",
    "    return cv2.equalizeHist(image)\n",
    "\n",
    "def mean_normalization(image):\n",
    "    '''\n",
    "    Normalize the values in the image to have mean 0.5\n",
    "    '''\n",
    "    # The function assumes that values are in the [0,1] range\n",
    "    image_ = image + (.5 - np.mean(image))\n",
    "    return np.maximum(np.minimum(image_, 1.), .0)\n",
    "\n",
    "def get_normal_fce(normalization):\n",
    "    '''\n",
    "    selects the corresponding normalizing function\n",
    "    '''\n",
    "    if normalization == 'HE':\n",
    "        return hist_equalization\n",
    "    if normalization == 'MEDIAN':\n",
    "        return median_normalization\n",
    "    if normalization == 'MEAN':\n",
    "        return mean_normalization\n",
    "    else:\n",
    "        error('normalization function was not picked')\n",
    "        return None\n",
    "\n",
    "def remove_uneven_illumination(img, blur_kernel_size=501, data_type='uint16'):\n",
    "    '''\n",
    "    uses LPF to remove uneven illumination\n",
    "    '''\n",
    "\n",
    "    img_f = img.astype(np.float32)\n",
    "    img_mean = np.mean(img_f)\n",
    "\n",
    "    img_blur = cv2.GaussianBlur(img_f, (blur_kernel_size, blur_kernel_size), 0)\n",
    "    if data_type == 'uint16':\n",
    "        result = np.maximum(np.minimum((img_f - img_blur) + img_mean, 65535), 0).astype(np.int32)\n",
    "        return result\n",
    "    elif datatype == 'uint8':\n",
    "        result = np.maximum(np.minimum((img_f - img_blur) + img_mean, 255), 0).astype(np.int32)\n",
    "        return result\n",
    "    else:\n",
    "        error('Unknown datatype {}'.format(data_type))\n",
    "        return None\n",
    "\n",
    "def read_instances(path, radious=5):\n",
    "    '''\n",
    "    Reads the instance segmentations and returns a binary mask and the marks for the detections.\n",
    "    '''\n",
    "    labels = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "    mi = labels.shape[0]\n",
    "    ni = labels.shape[1]\n",
    "\n",
    "    if np.sum(labels) > 0:\n",
    "        # Obtain the instances\n",
    "        components = np.unique(labels)\n",
    "        n_comp = len(components) - 1\n",
    "        detection_marks = np.zeros((n_comp, mi, ni))\n",
    "        for c in range(n_comp):\n",
    "            # Only keeps current object.\n",
    "            tmp = (labels == components[c + 1])\n",
    "            # Transform the label with erosion\n",
    "            diskelem = skimage.morphology.disk(radious)\n",
    "            detection_marks[c] = skimage.morphology.binary_erosion(tmp, diskelem)\n",
    "        detection_marks = np.sum(detection_marks, axis=0)\n",
    "        detection_marks = detection_marks.reshape((mi, ni))\n",
    "    else:\n",
    "        detection_marks = np.zeros((mi, ni))\n",
    "    labels[labels > 0] = 1\n",
    "    return labels, detection_marks\n",
    "\n",
    "def read_input_image(path, normalization='MEAN', uneven_illumination=False):\n",
    "  normalization_fce = get_normal_fce(normalization)\n",
    "  im = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "  \n",
    "  mi = im.shape[0]\n",
    "  ni = im.shape[1]\n",
    "  if im is None:\n",
    "      print('image {} was not loaded'.format(name))\n",
    "\n",
    "  if uneven_illumination:\n",
    "      # o = np.minimum(o, 255).astype(np.uint8) # Original from MU-Lux_CZ\n",
    "      im = remove_uneven_illumination(im, data_type=im.dtype.name)\n",
    "\n",
    "  # convert values to the [0,1] range\n",
    "  if im.dtype.name == 'uint16':\n",
    "    im = im.astype(np.float32)\n",
    "    im = im / 65535\n",
    "  else:\n",
    "    im = im.astype(np.float32)\n",
    "    im = im / 255\n",
    "  \n",
    "  im = normalization_fce(im)\n",
    "\n",
    "  return im\n",
    "\n",
    "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,list_IDs, dataset_path, rotation_range, width_shift_range, height_shift_range, shear_range, zoom_range, horizontal_flip, fill_mode, dim_input=None, dim_output=None, pdf=1, module=\"train\", instance_radious=5, patch_batch=1, batch_size=5, shuffle=True):\n",
    "        # Suffle is used to take everytime a different sample from the list in a random way so the training order differs. \n",
    "        #     # we create two instances with the same arguments\n",
    "        # 'Initialization'\n",
    "\n",
    "            \n",
    "        self.list_IDs = list_IDs\n",
    "        \n",
    "#         if module=='Train':\n",
    "#             self.dataset_path = '../data/training/'\n",
    "#         else:\n",
    "#             self.dataset_path = '../data/test/'\n",
    "\n",
    "        self.dataset_path = dataset_path   \n",
    "        self.rotation_range = rotation_range\n",
    "        self.width_shift_range = width_shift_range\n",
    "        self.height_shift_range = height_shift_range\n",
    "        self.shear_range = shear_range\n",
    "        self.zoom_range = zoom_range\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        \n",
    "        self.fill_mode = fill_mode       \n",
    "        self.pdf = pdf\n",
    "        self.module = module # \"Train\" or \"Test\"    \n",
    "        self.instance_radious = instance_radious    \n",
    "#         self.data_format = data_format\n",
    "        if dim_input == None:\n",
    "            dim_input =(512,512)\n",
    "        if dim_output == None:\n",
    "            dim_output=(512,512)\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.patch_batch = patch_batch\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle # #\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        # print(index)\n",
    "        # print(self.indexes)\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # print(indexes)\n",
    "        # Generate data\n",
    "        x,y = self.__data_generation(list_IDs_temp)\n",
    "        return x,y\n",
    "        # Generate data\n",
    "        # x = self.__data_generation(list_IDs_temp)\n",
    "        # return x\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)    \n",
    "    # crop patches\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \n",
    "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        \n",
    "        # Initialization\n",
    "        x = np.empty((self.batch_size*self.patch_batch, self.dim_input[0],  self.dim_input[1],1))\n",
    "        w = np.empty((self.batch_size*self.patch_batch, self.dim_input[0],  self.dim_input[1],1))\n",
    "        y = np.empty((self.batch_size*self.patch_batch, self.dim_output[0],  self.dim_output[1], 1))\n",
    "\n",
    "        # Generate data\n",
    "        j = 0\n",
    "        if self.module == 'train':\n",
    "          for patch in range(self.patch_batch):                        \n",
    "              for i, ID in enumerate(list_IDs_temp):\n",
    "                # Store sample\n",
    "                # print(ID)\n",
    "                # ID = list_IDs_temp[0]\n",
    "                ID = ID.split('_')[-1].split('.')[0]\n",
    "                # print(ID)\n",
    "                # print(\"{0}/inputs/raw_{1}.tif\".format(self.dataset_path, ID))\n",
    "                aux_x = read_input_image(\"{0}/inputs/raw_{1}.tif\".format(self.dataset_path, ID))\n",
    "                aux_y,aux_y_marks = read_instances(\"{0}/labels/instance_ids_{1}.tif\".format(self.dataset_path, ID), radious=self.instance_radious)\n",
    "                aux_y_weights = cv2.imread(\"{0}/weights/instance_ids_{1}_weight.tif\".format(self.dataset_path, ID), cv2.IMREAD_ANYDEPTH)\n",
    "                augmented_x, augmented_y, augmented_y_marks, augmented_weights = data_augmentation_weightedmaps(aux_x, aux_y, aux_y_marks, aux_y_weights, self)             \n",
    "                x[j,:,:,0] = augmented_x\n",
    "                w[j,:,:,0] = augmented_weights\n",
    "\n",
    "                # Store ground truth\n",
    "                augmented_y[augmented_y>0] = 1\n",
    "                # augmented_y_marks[augmented_y_marks>0] = 1\n",
    "                y[j,:,:,0] = augmented_y\n",
    "              # y[j,:,:,1] = 1-augmented_y\n",
    "              # y[j,:,:,2] = augmented_y_marks\n",
    "              # y[j,:,:,3] = 1-augmented_y_marks\n",
    "                j += 1 \n",
    "\n",
    "        elif self.module == 'test':\n",
    "          for patch in range(self.patch_batch):                        \n",
    "              for i, ID in enumerate(list_IDs_temp):\n",
    "                ID = ID.split('_')[-1].split('.')[0]\n",
    "                # print(\"{0}/inputs/raw_{1}.tif\".format(self.dataset_path, ID))\n",
    "                aux_x = read_input_image(\"{0}/inputs/raw_{1}.tif\".format(self.dataset_path, ID))\n",
    "                aux_y,aux_y_marks = read_instances(\"{0}/labels/instance_ids_{1}.tif\".format(self.dataset_path, ID), radious=self.instance_radious)\n",
    "                # Get random crops\n",
    "\n",
    "                input_im,output_labels,output_marks,weights = random_crop(aux_x,aux_y,aux_y_marks,aux_y, (self.dim_input[0],  self.dim_input[1]), (self.dim_output[0],  self.dim_output[1]), pdf=self.pdf)\n",
    "                # plt.figure(figsize=(15,10))\n",
    "                # plt.subplot(2,2,1)\n",
    "                # plt.imshow(aux_x)\n",
    "                # plt.subplot(2,2,2)\n",
    "                # plt.imshow(aux_y)\n",
    "                # plt.subplot(2,2,3)\n",
    "                # plt.imshow(input_im)\n",
    "                # plt.subplot(2,2,4)\n",
    "                # plt.imshow(output_labels)\n",
    "                # plt.show()\n",
    "                x[j,:,:,0] = input_im\n",
    "                w[j,:,:,0] = weights\n",
    "\n",
    "                # Store ground truth\n",
    "                output_labels[output_labels>0] = 1\n",
    "                output_marks[output_marks>0] = 1\n",
    "                y[j,:,:,0] = output_labels\n",
    "              # y[j,:,:,1] = 1-output_labels\n",
    "              # y[j,:,:,2] = output_marks\n",
    "              # y[j,:,:,3] = 1-output_marks\n",
    "                j += 1\n",
    "\n",
    "        x = x.astype(np.float32)\n",
    "        w = w.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        # return [x,w,y]\n",
    "        return [x,w],y\n",
    "        # return x,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kcx_GRUOYhrM"
   },
   "source": [
    "## Try your data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpea8F7dZIUj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "DATASET_PATH = \"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/data/train/stack2im\"\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'pdf': 5000,\n",
    "          'fill_mode': 'reflect',\n",
    "          'patch_batch': 2,\n",
    "          'batch_size': 5,\n",
    "          'module': 'train'}\n",
    "files4training = os.listdir(DATASET_PATH + '/inputs/')\n",
    "files4training.sort()\n",
    "\n",
    "partition ={'train': files4training}\n",
    "self = DataGenerator(partition['train'],**params)\n",
    "\n",
    "data = self.__getitem__(1)\n",
    "\n",
    "# ID = 'raw_001.tif'\n",
    "\n",
    "# aux_x = read_input_image(self.dataset_path + '/inputs/'+ ID)\n",
    "\n",
    "# aux_y,aux_y_marks = read_instances(self.dataset_path + '/labels/'+ ID, radious=self.instance_radious)\n",
    "\n",
    "# aux_y_weights = cv2.imread(self.dataset_path + '/weights/'+ ID, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "# seed = np.random.randint(100000)\n",
    "# random_crop_size_input = (512,512)\n",
    "# random_crop_size_output = (512,512)\n",
    "# c = 0\n",
    "# input_im,output_labels,output_marks,weights = random_crop(aux_x, aux_y,aux_y_marks,aux_y_weights, random_crop_size_input, random_crop_size_output, sync_seed=seed)\n",
    "for i in range(data[1].shape[0]):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.subplot(2,2,1)\n",
    "  plt.imshow(data[0][0][i,:,:,0])\n",
    "  plt.subplot(2,2,2)\n",
    "  plt.imshow(data[0][1][i,:,:,0])\n",
    "  plt.subplot(2,2,3)\n",
    "  plt.imshow(data[1][i,:,:,0])\n",
    "  plt.subplot(2,2,4)\n",
    "  plt.imshow(data[1][i,:,:,0])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RM466r6JYlVw"
   },
   "source": [
    "# Define a U-Net with weighted loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7L_YbInTpps"
   },
   "source": [
    "## Define weighted losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAvJ06LnpCOs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# weight: weighted tensor(same shape with mask image)\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weight - 1.) * y_true) * \\\n",
    "    (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight * weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(11, 11), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + \\\n",
    "    weighted_dice_loss(y_true, y_pred, weight)\n",
    "    return loss\n",
    "\n",
    "def binary_crossentropy_weighted(weights):\n",
    "\n",
    "  def loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weights - 1.) * y_true) * \\\n",
    "    (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJLy8xz9ZMpd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "# def binary_crossentropy_weighted(weights):\n",
    "#   \"\"\"\n",
    "#   Custom binary cross entropy loss. The weights are used to multiply\n",
    "#   the results of the usual cross-entropy loss in order to give more weight\n",
    "#   to areas between cells close to one another.\n",
    "\n",
    "#   The variable 'weights' refers to input weight-maps.\n",
    "#   \"\"\"\n",
    "\n",
    "#   def loss(y_true, y_pred):\n",
    "#     return K.mean(weights * K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "#   return loss\n",
    "\n",
    "# def weighted_binary_loss(X):\n",
    "#     y_pred, weights, y_true = X\n",
    "#     loss = tensorflow.keras.losses.binary_crossentropy(y_pred, y_true)\n",
    "#     loss = tensorflow.keras.layers.multiply([loss, weights])\n",
    "#     return loss\n",
    "\n",
    "# def identity_loss(y_true, y_pred):\n",
    "#     return K.mean(y_pred, axis=-1)\n",
    "\n",
    "def weighted_BCE(weights):\n",
    "  # https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/\n",
    "  def convert_to_logits(y_pred):\n",
    "      # see https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/backend.py#L3525\n",
    "      y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "      return K.log(y_pred / (1 - y_pred))\n",
    "\n",
    "  def loss(y_true, y_pred):\n",
    "    y_pred = convert_to_logits(y_pred)\n",
    "    # pos_weight = beta / (1 - beta)\n",
    "    # pos_weight = beta / (1 - beta) + tf.exp(-tf.pow(weights, 2))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, labels=y_true, pos_weight=weights)\n",
    "\n",
    "    # or reduce_sum and/or axis=-1\n",
    "    # return tf.reduce_mean(loss * (1 - beta))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "  return loss\n",
    "\n",
    "# def weighted_mse(weights):\n",
    "#   \"\"\"\n",
    "#   Custom mean squared error loss. The weights are used to multiply\n",
    "#   the results of the usual mse loss in order to give more weight\n",
    "#   to areas between cells close to one another or to the contours\n",
    "\n",
    "#   The variable 'weights' refers to input weight-maps.\n",
    "#   \"\"\"\n",
    "#   def loss(y_true, y_pred):\n",
    "#     S = K.sum(weights * K.square(y_pred-y_true), axis=-1)\n",
    "#     factor = K.sum(0.5*weights,axis=-1)\n",
    "#     return K.sum(S / factor, axis=-1)\n",
    "\n",
    "#   return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "He27NedUBaW9"
   },
   "source": [
    "## first way of getting a unet with tf 2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAFuWgtQkPRP"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_squared_error\n",
    "\n",
    "def create_model(mi=256, ni=256, lr = 0.0001):\n",
    "  \"\"\"\n",
    "  Weighted U-Net architecture.\n",
    "\n",
    "  The tuple 'input_size' corresponds to the size of the input images and labels.\n",
    "  Default value set to (512, 512, 1) (input images size is 512x512).\n",
    "  \"\"\"\n",
    "  # TODO: change if using `channels_first` image data format\n",
    "  input_img = Input(shape=(mi, ni, 1))\n",
    "  # Get weights.\n",
    "  # weights = Input(shape=(mi, ni, 1))\n",
    "  # Downsampling path\n",
    "  c1e = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(input_img)\n",
    "  c1 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c1e)\n",
    "  p1 = MaxPooling2D((2, 2), padding='same')(c1)\n",
    "\n",
    "  c2e = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "  c2 = Conv2D(64, (3, 3), activation='elu',  kernel_initializer='he_normal', padding='same')(c2e)\n",
    "  p2 = MaxPooling2D((2, 2), padding='same')(c2)\n",
    "\n",
    "  c3e = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "  c3 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c3e)\n",
    "  p3 = MaxPooling2D((2, 2), padding='same')(c3)\n",
    "\n",
    "  c4e = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "  c4 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c4e)\n",
    "  p4 = MaxPooling2D((2, 2), padding='same')(c4)\n",
    "\n",
    "  c5e = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "  c5 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c5e)\n",
    "\n",
    "  # Upsampling path\n",
    "  u4 = UpSampling2D((2, 2), interpolation='bilinear')(c5)\n",
    "  a4 = Concatenate(axis=3)([u4, c4])\n",
    "  c6e = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(a4)\n",
    "  c6 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c6e)\n",
    "\n",
    "  u3 = UpSampling2D((2, 2), interpolation='bilinear')(c6)\n",
    "  a3 = Concatenate(axis=3)([u3, c3])\n",
    "  c7e = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(a3)\n",
    "  c7 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c7e)\n",
    "\n",
    "  u2 = UpSampling2D((2, 2), interpolation='bilinear')(c7)\n",
    "  a2 = Concatenate(axis=3)([u2, c2])\n",
    "  c8e = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(a2)\n",
    "  c8 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c8e)\n",
    "\n",
    "  u1 = UpSampling2D((2, 2), interpolation='bilinear')(c8)\n",
    "  a1 = Concatenate(axis=3)([u1, c1])\n",
    "  c9 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(a1)\n",
    "\n",
    "  c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "  # Softmax\n",
    "  # markers = Conv2D(2, (1, 1), activation='softmax', padding='same')(c10)\n",
    "  c11 = Conv2D(2, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same')(c10)\n",
    "  cell_mask = Conv2D(1, (1, 1), activation='softmax', kernel_initializer='he_normal', padding='same')(c11)\n",
    "  # output = Concatenate(axis=3, name='output')([markers, cell_mask])\n",
    "\n",
    "  # Specify input (image + weights) and output.\n",
    "  # model = Model(inputs=[input_img,weights], outputs=output)\n",
    "  model = Model(inputs=input_img, outputs=cell_mask)\n",
    "  # Use Adam optimizer, custom weighted mean squared error loss and specify metrics\n",
    "  # Also use weights inside the loss function.\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  # model = Model(input_img, output)\n",
    "  print('U-Net model was created')\n",
    "  return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zVBJ8wIBPRE"
   },
   "source": [
    "## second way of getting a unet with tf 2.0.0\n",
    "https://www.kaggle.com/advaitsave/tensorflow-2-nuclei-segmentation-unet\n",
    "\n",
    "## binary weighted loss example\n",
    "https://stackoverflow.com/questions/48555820/keras-binary-segmentation-add-weight-to-loss-function/48577360\n",
    "\n",
    "https://stackoverflow.com/questions/55213599/u-net-with-pixel-wise-weighted-cross-entropy-input-dimension-errors\n",
    "\n",
    "https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJhO_hdeA4ga"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model(mi=256, ni=256, lr = 0.0001):\n",
    "  \"\"\"\n",
    "  Weighted U-Net architecture.\n",
    "\n",
    "  The tuple 'input_size' corresponds to the size of the input images and labels.\n",
    "  Default value set to (512, 512, 1) (input images size is 512x512).\n",
    "  \"\"\"\n",
    "\n",
    "  # Build U-Net model\n",
    "  inputs = Input((mi,ni,1))\n",
    "  # Get weights.\n",
    "  # weights = Input((mi, ni, 1))\n",
    "  # # Get true masks\n",
    "  # true_masks = Input((mi, ni, 1))\n",
    "\n",
    "  c1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
    "  c1 = BatchNormalization()(c1)\n",
    "  c1 = Dropout(0.1) (c1)\n",
    "  c1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "  c1 = BatchNormalization()(c1)\n",
    "  p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "  c2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "  c2 = BatchNormalization()(c2)\n",
    "  c2 = Dropout(0.1) (c2)\n",
    "  c2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "  c2 = BatchNormalization()(c2)\n",
    "  p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "  c3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "  c3 = BatchNormalization()(c3)\n",
    "  c3 = Dropout(0.2) (c3)\n",
    "  c3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "  c3 = BatchNormalization()(c3)\n",
    "  p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "  c4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "  c4 = BatchNormalization()(c4)\n",
    "  c4 = Dropout(0.2) (c4)\n",
    "  c4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "  c4 = BatchNormalization()(c4)\n",
    "  p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "  c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "  c5 = BatchNormalization()(c5)\n",
    "  c5 = Dropout(0.3) (c5)\n",
    "  c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "  c5 = BatchNormalization()(c5)\n",
    "\n",
    "  u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "  u6 = concatenate([u6, c4])\n",
    "  c6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "  c6 = BatchNormalization()(c6)\n",
    "  c6 = Dropout(0.2) (c6)\n",
    "  c6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "  c6 = BatchNormalization()(c6)\n",
    "\n",
    "  u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "  u7 = concatenate([u7, c3])\n",
    "  c7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "  c7 = BatchNormalization()(c7)\n",
    "  c7 = Dropout(0.2) (c7)\n",
    "  c7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "  c7 = BatchNormalization()(c7)\n",
    "\n",
    "  u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "  u8 = concatenate([u8, c2])\n",
    "  c8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "  c8 = BatchNormalization()(c8)\n",
    "  c8 = Dropout(0.1) (c8)\n",
    "  c8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "  c8 = BatchNormalization()(c8)\n",
    "\n",
    "  u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "  u9 = concatenate([u9, c1], axis=3)\n",
    "  c9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "  c9 = BatchNormalization()(c9)\n",
    "  c9 = Dropout(0.1) (c9)\n",
    "  c9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "  c9 = BatchNormalization()(c9)\n",
    "\n",
    "  outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "  # loss = partial(loss_function, weights)\n",
    "  model = Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(optimizer=Adam(learning_rate=lr, name='adam'), loss=weighted_bce_dice_loss, metrics=['accuracy'])\n",
    "\n",
    "  # Use a lambda layer to calculate the weighted loss \n",
    "  # loss = Lambda(weighted_binary_loss, output_shape=(mi, ni, 1))([outputs, weights, true_masks])\n",
    "  # model = Model(inputs=[inputs, weights, true_masks], outputs=loss)\n",
    "  # model.compile(optimizer=Adam(learning_rate=lr, name='adam'), loss=identity_loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  # Specify input (image + weights) and output.\n",
    "  # model = Model(inputs=[inputs,weights], outputs=outputs)\n",
    "  # model.compile(optimizer=Adam(learning_rate=lr,name='adam'), loss=binary_crossentropy_weighted(weights), metrics=['accuracy'])\n",
    "  # model = Model(inputs=[inputs], outputs=[outputs])\n",
    "  # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  print('U-Net model was created')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvvBoPrQYq9A"
   },
   "source": [
    "# Use a common U-Net structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 980,
     "status": "error",
     "timestamp": 1585262474080,
     "user": {
      "displayName": "ESTIBALIZ GOMEZ DE MARISCAL",
      "photoUrl": "",
      "userId": "04592796515262324641"
     },
     "user_tz": -60
    },
    "id": "S_Rx6mu5Ycsg",
    "outputId": "9e6f7f0c-e68e-4713-d255-946e36bef41d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-99db822221df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'elu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'he_normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m'It looks like you are trying to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;34m'a version of multi-backend Keras that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m'does not support TensorFlow 2.0. We recommend '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14."
     ]
    }
   ],
   "source": [
    "# Create U-Net for super-resolution\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, UpSampling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "inputs = Input((512, 512, 1))\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "p1 = AveragePooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = AveragePooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "p3 = AveragePooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "\n",
    "u5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c4)\n",
    "u5 = concatenate([u5, c3])\n",
    "c5 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u5)\n",
    "c5 = Dropout(0.2) (c5)\n",
    "c5 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c2])\n",
    "c6 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = Dropout(0.1) (c6)\n",
    "c6 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c1], axis=3)\n",
    "c7 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "c7 = Dropout(0.1) (c7)\n",
    "c7 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "outputs = Conv2D(2, (1, 1), activation='sigmoid') (c7)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "# compile the model with RMSProp as optimizer, MSE as loss function and MAE as metric\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['mean_absolute_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLCebQ1AgnyB"
   },
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ck-qQDAUfiwv"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PAZXyvsb16V"
   },
   "source": [
    "## Evaluate the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2IMyBCl46Zf"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/data/train/stack2im\"\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'pdf': 5000,\n",
    "          'fill_mode': 'reflect',          \n",
    "          'dim_input': (256,256),\n",
    "          'dim_output': (256,256),\n",
    "          'patch_batch': 2,\n",
    "          'batch_size': 1\n",
    "          }\n",
    "files4training = os.listdir(DATASET_PATH + '/inputs/')\n",
    "files4training.sort()\n",
    "\n",
    "partition ={'train': files4training}\n",
    "\n",
    "# Generators\n",
    "self = DataGenerator(partition['train'],**params)\n",
    "data = self.__getitem__(30)\n",
    "for i in range(data[1].shape[0]):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.subplot(2,2,1)\n",
    "  plt.imshow(data[0][0][i,:,:,0])\n",
    "  plt.subplot(2,2,2)\n",
    "  plt.imshow(data[0][1][i,:,:,0])\n",
    "  plt.subplot(2,2,3)\n",
    "  plt.imshow(data[1][i,:,:,0])\n",
    "  plt.subplot(2,2,4)\n",
    "  plt.imshow(data[1][i,:,:,0])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWSB8_dnb_xN"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ3w1Jq8-CIN"
   },
   "outputs": [],
   "source": [
    "model = create_model(mi=256, ni=256, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5UmD-0ROnuk"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/checkpoints/2dUnet_weights_00004.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEMj2o5kcRxn"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25924556,
     "status": "error",
     "timestamp": 1585678137416,
     "user": {
      "displayName": "ESTIBALIZ GOMEZ DE MARISCAL",
      "photoUrl": "",
      "userId": "04592796515262324641"
     },
     "user_tz": -120
    },
    "id": "Lai1Ik6ZkGJF",
    "outputId": "fb5fa9d3-98b0-4aac-d40d-d1ed72525f6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "666/666 [==============================] - 5872s 9s/step - loss: 0.2403 - accuracy: 0.9945\n",
      "Epoch 2/10\n",
      "666/666 [==============================] - 1717s 3s/step - loss: 0.2498 - accuracy: 0.9938\n",
      "Epoch 3/10\n",
      "666/666 [==============================] - 1617s 2s/step - loss: 0.2173 - accuracy: 0.9945\n",
      "Epoch 4/10\n",
      "666/666 [==============================] - 1619s 2s/step - loss: 0.2282 - accuracy: 0.9942\n",
      "Epoch 5/10\n",
      "666/666 [==============================] - 1605s 2s/step - loss: 0.2351 - accuracy: 0.9940\n",
      "Epoch 6/10\n",
      "666/666 [==============================] - 1582s 2s/step - loss: 0.2242 - accuracy: 0.9934\n",
      "Epoch 7/10\n",
      "666/666 [==============================] - 1576s 2s/step - loss: 0.2153 - accuracy: 0.9951\n",
      "Epoch 8/10\n",
      "666/666 [==============================] - 1610s 2s/step - loss: 0.1781 - accuracy: 0.9951\n",
      "Epoch 9/10\n",
      "666/666 [==============================] - 1641s 2s/step - loss: 0.1836 - accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "666/666 [==============================] - 1722s 3s/step - loss: 0.1696 - accuracy: 0.9949\n",
      "Epoch 1/10\n",
      "666/666 [==============================] - 1740s 3s/step - loss: 0.1696 - accuracy: 0.9953\n",
      "Epoch 2/10\n",
      "666/666 [==============================] - 1791s 3s/step - loss: 0.1631 - accuracy: 0.9954\n",
      "Epoch 3/10\n",
      "647/666 [============================>.] - ETA: 50s - loss: 0.1659 - accuracy: 0.9955"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e65f64717c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0;31m# model.fit(X_train, Y_train, batch_size=1, epochs=1, callbacks=callbacks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;31m# model.fit(X_train,Y_train, batch_size=1, epochs=1, callbacks=callbacks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# data = test_generator.__getitem__(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# X_train = data[0][0] # [0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "# from my_classes import DataGenerator\n",
    "\n",
    "DATASET_PATH = \"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/data/train/stack2im\"\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'pdf': 5000,\n",
    "          'fill_mode': 'reflect',          \n",
    "          'dim_input': (256,256),\n",
    "          'dim_output': (256,256),\n",
    "          'patch_batch': 1,\n",
    "          'batch_size': 1,\n",
    "          'module': 'train'\n",
    "          }\n",
    "files4training = os.listdir(DATASET_PATH + '/inputs/')\n",
    "files4training.sort()\n",
    "\n",
    "TEST_PATH = \"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/data/test/stack2im\"\n",
    "params_test = {'dataset_path': TEST_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'pdf': 5000,\n",
    "          'fill_mode': 'reflect',          \n",
    "          'dim_input': (256,256),\n",
    "          'dim_output': (256,256),\n",
    "          'patch_batch': 1,\n",
    "          'batch_size': 10,\n",
    "          'module': 'test'\n",
    "          }\n",
    "files4test = os.listdir(TEST_PATH + '/inputs/')\n",
    "files4test.sort()\n",
    "\n",
    "partition ={'train': files4training,\n",
    "            'test': files4test}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'],**params)\n",
    "test_generator = DataGenerator(partition['test'],**params_test)\n",
    "test_data = test_generator.__getitem__(1)\n",
    "for j in range(len(test_data[1])):\n",
    "    sitk.WriteImage(sitk.GetImageFromArray(test_data[1][j,:,:,0].astype(np.uint8)),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/gt_{0}.tif\".format(np.str(j)))\n",
    "    sitk.WriteImage(sitk.GetImageFromArray(test_data[0][0][j,:,:,0]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/input_{0}.tif\".format(np.str(j)))\n",
    "\n",
    "\n",
    "\n",
    "# # Train model on dataset\n",
    "# model = create_model(mi=256, ni=256, lr = 0.0001)\n",
    "# model.optimizer.get_config()\n",
    "# model.summary()\n",
    "\n",
    "callbacks = []\n",
    "checkpoint = ModelCheckpoint('/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/checkpoints/2dUnet_weights_{epoch:05d}.hdf5', monitor='loss',\n",
    "                             save_weights_only=True, mode='auto', save_freq=20)\n",
    "if not os.path.exists('/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/checkpoints/'):\n",
    "        os.makedirs('/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/checkpoints/')\n",
    "checkpoint.set_model(model)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/logs', write_graph=True,# write_grads=True, batch_size=1, \n",
    "                          write_images=False, update_freq=10)\n",
    "tensorboard.set_model(model)\n",
    "callbacks.append(tensorboard)\n",
    "\n",
    "# numEpochs = 50\n",
    "# earlystopper = EarlyStopping(patience=5, verbose=1, restore_best_weights=True)\n",
    "# earlystopper.set_model(model)\n",
    "# callbacks.append(earlystopper)\n",
    "\n",
    "reducelearning = ReduceLROnPlateau(monitor='lr', factor=0.1, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "reducelearning.set_model(model)\n",
    "callbacks.append(reducelearning)\n",
    "\n",
    "# Fit model\n",
    "# earlystopper = EarlyStopping(patience=15, verbose=1)\n",
    "# checkpointer = ModelCheckpoint('model_unet_checkpoint.h5', verbose=1, save_best_only=True)\n",
    "for i in range(19):\n",
    "  # data = training_generator.__getitem__(1)\n",
    "  # X_train = data[0][0] # [0]\n",
    "  # Y_train = data[1]\n",
    "  # del data\n",
    "  # model.fit(X_train, Y_train, batch_size=1, epochs=1, callbacks=callbacks)\n",
    "  # model.fit(X_train,Y_train, batch_size=1, epochs=1, callbacks=callbacks)\n",
    "  model.fit_generator(generator=training_generator, epochs=10, callbacks=callbacks) \n",
    "  # data = test_generator.__getitem__(1)\n",
    "  # X_train = data[0][0] # [0]\n",
    "  # Y_train = data[1]\n",
    "  pred = model.predict(test_data[0][0])\n",
    "  for j in range(pred.shape[0]):\n",
    "    sitk.WriteImage(sitk.GetImageFromArray(pred[j,:,:,0]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/prediction_CHANNEL1_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "    # sitk.WriteImage(sitk.GetImageFromArray(pred[j,:,:,1]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/prediction_epoch_CHANNEL2_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "    # sitk.WriteImage(sitk.GetImageFromArray(Y_train[j,:,:,0].astype(np.uint8)),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/gt_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "    # sitk.WriteImage(sitk.GetImageFromArray(X_train[j,:,:,0]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/input_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "  del pred\n",
    "  # del X_train\n",
    "  # del Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rlAfn6oyHmV"
   },
   "outputs": [],
   "source": [
    "  pred = model.predict(test_data[0][0])\n",
    "  for j in range(pred.shape[0]):\n",
    "    sitk.WriteImage(sitk.GetImageFromArray(pred[j,:,:,0]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/prediction_CHANNEL1_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "    # sitk.WriteImage(sitk.GetImageFromArray(pred[j,:,:,1]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/prediction_epoch_CHANNEL2_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "    # sitk.WriteImage(sitk.GetImageFromArray(Y_train[j,:,:,0].astype(np.uint8)),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/gt_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "    # sitk.WriteImage(sitk.GetImageFromArray(X_train[j,:,:,0]),\"/content/drive/My Drive/Projectos/3D-PROTUCEL/Code/MU_Lux_CZ/binary001/input_{0}_{1}.tif\".format(np.str(i), np.str(j)))\n",
    "  del pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQapfDEOPUr6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6394,
     "status": "ok",
     "timestamp": 1585324897991,
     "user": {
      "displayName": "ESTIBALIZ GOMEZ DE MARISCAL",
      "photoUrl": "",
      "userId": "04592796515262324641"
     },
     "user_tz": -60
    },
    "id": "-S5PlZovEI3q",
    "outputId": "8c10c4b5-6bd7-48e7-bb0b-54bb50494a9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: 'shape' argument should be used instead of 'dims'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4fd2ef9c50>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAADiCAYAAAC1HIe6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEDl3IYSoIf8CEr9AVVJb9ZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "  # plt.figure()\n",
    "  # plt.subplot(2,4,1)\n",
    "  # plt.imshow(pred[0,:,:,0])\n",
    "  # plt.subplot(2,4,2)\n",
    "  # plt.imshow(data[1][0,:,:,0])\n",
    "  # plt.subplot(2,4,3)\n",
    "  # plt.imshow(pred[1,:,:,0])\n",
    "  # plt.subplot(2,4,4)\n",
    "  # plt.imshow(data[1][1,:,:,0])\n",
    "  # plt.subplot(2,4,5)\n",
    "  # plt.imshow(pred[2,:,:,0])\n",
    "  # plt.subplot(2,4,6)\n",
    "  # plt.imshow(data[1][2,:,:,0])\n",
    "  # plt.subplot(2,4,7)\n",
    "  # plt.imshow(pred[3,:,:,0])\n",
    "  # plt.subplot(2,4,8)\n",
    "  # plt.imshow(data[1][3,:,:,0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data = training_generator.__getitem__(30)\n",
    "pred = model.predict(data[0][:4])\n",
    "plt.figure()\n",
    "plt.subplot(2,4,1)\n",
    "plt.imshow(pred[0,:,:,1])\n",
    "plt.subplot(2,4,2)\n",
    "plt.imshow(data[1][0,:,:,0])\n",
    "plt.subplot(2,4,3)\n",
    "plt.imshow(pred[1,:,:,1])\n",
    "plt.subplot(2,4,4)\n",
    "plt.imshow(data[1][1,:,:,0])\n",
    "plt.subplot(2,4,5)\n",
    "plt.imshow(pred[2,:,:,1])\n",
    "plt.subplot(2,4,6)\n",
    "plt.imshow(data[1][2,:,:,0])\n",
    "plt.subplot(2,4,7)\n",
    "plt.imshow(pred[3,:,:,1])\n",
    "plt.subplot(2,4,8)\n",
    "plt.imshow(data[1][3,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QUKyuQbpK85"
   },
   "outputs": [],
   "source": [
    "p = model.predict(data[0])\n",
    "for i in range(data[1].shape[0]):\n",
    "\n",
    "  plt.imshow(p[i,:,:,1])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIc3MlGW5CKI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, BaseLogger, Callback, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "# from my_classes import DataGenerator\n",
    "\n",
    "DATASET_PATH = ('/home/esgomezm/Projects/3D-PROTUCEL/data/training/')\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'fill_mode': 'reflect',\n",
    "          'patch_batch': 5}\n",
    "import glob\n",
    "files4training = glob.glob(DATASET_PATH + 'input/input/*.tif')\n",
    "for i in range(len(files4training)):\n",
    "    files4training[i] = files4training[i][len(DATASET_PATH+ 'input/input/'):]\n",
    "    \n",
    "partition ={'train': files4training}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'],**params)\n",
    "\n",
    "# Train model on dataset\n",
    "model = get_unet(220,220,21,3,0.0001,'binary_crossentropy', 'Adam', 'accuracy', 32, 1254)\n",
    "\n",
    "callbacks = []\n",
    "checkpoint = ModelCheckpoint('/home/esgomezm/Projects/3D-PROTUCEL/Code/binary001/checkpoints/2dUnet_weights_{epoch:05d}.hdf5', monitor='loss',\n",
    "                             save_weights_only=True, mode='auto', period=20)\n",
    "checkpoint.set_model(model)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./binary001/logs', batch_size=1, write_graph=True, write_grads=True, \n",
    "                          write_images=False, update_freq=10)\n",
    "tensorboard.set_model(model)\n",
    "callbacks.append(tensorboard)\n",
    "\n",
    "reducelearning = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=50, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "reducelearning.set_model(model)\n",
    "callbacks.append(reducelearning)\n",
    "\n",
    "model.fit_generator(generator=training_generator, epochs=1000, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKLq6cVl5CKb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from convolutional_networks import get_unet, get_unet_nopadding\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, BaseLogger, Callback, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "# from my_classes import DataGenerator\n",
    "\n",
    "DATASET_PATH = ('/home/esgomezm/Projects/3D-PROTUCEL/data/training/')\n",
    "dim_input=(220,220,21)\n",
    "x = dim_input[0]\n",
    "y = 2*(2*(2*(2*(0.5*(0.5*(0.5*(0.5*(x-4)-4)-4)-4)-4)-4)-4)-4)-4\n",
    "y = np.int(y)\n",
    "dim_output=(y,y,1)\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'fill_mode': 'reflect',\n",
    "          'patch_batch': 5,\n",
    "          'dim_input': dim_input,\n",
    "          'dim_output': dim_output}\n",
    "import glob\n",
    "files4training = glob.glob(DATASET_PATH + 'input/input/*.tif')\n",
    "for i in range(len(files4training)):\n",
    "    files4training[i] = files4training[i][len(DATASET_PATH+ 'input/input/'):]\n",
    "    \n",
    "partition ={'train': files4training}\n",
    "\n",
    "# Train model on dataset\n",
    "model = get_unet_nopadding(220,220,21,3,0.0001,'binary_crossentropy', 'Adam', 'accuracy', 32, 1254)\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'],**params)\n",
    "\n",
    "callbacks = []\n",
    "checkpoint = ModelCheckpoint('/home/esgomezm/Projects/3D-PROTUCEL/Code/binary001/checkpoints/2dUnet_nopadding_weights_01_{epoch:05d}.hdf5', monitor='loss',\n",
    "                             save_weights_only=False, mode='auto', period=20)\n",
    "checkpoint.set_model(model)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./binary001/logs', batch_size=1, write_graph=True, write_grads=True, \n",
    "                          write_images=False, update_freq=10)\n",
    "tensorboard.set_model(model)\n",
    "callbacks.append(tensorboard)\n",
    "\n",
    "reducelearning = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=50, verbose=2, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "reducelearning.set_model(model)\n",
    "callbacks.append(reducelearning)\n",
    "\n",
    "# model.load_weights(\"./binary001/checkpoints/2dUnet_weights_00160.hdf5\")\n",
    "\n",
    "# model.fit_generator(generator=training_generator, epochs=2, callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZwSuHoY5CKj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, BaseLogger, Callback, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "# from my_classes import DataGenerator\n",
    "\n",
    "DATASET_PATH = ('/home/esgomezm/Projects/3D-PROTUCEL/data/training/')\n",
    "dim_input=(220,220,21)\n",
    "x = dim_input[0]\n",
    "y = 2*(2*(2*(2*(0.5*(0.5*(0.5*(0.5*(x-4)-4)-4)-4)-4)-4)-4)-4)-4\n",
    "dim_output=(y,y,1)\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'fill_mode': 'reflect',\n",
    "          'patch_batch': 5\n",
    "          'dim_input': dim_input,\n",
    "          'dim_ouput': dim_ouput}\n",
    "import glob\n",
    "files4training = glob.glob(DATASET_PATH + 'input/input/*.tif')\n",
    "for i in range(len(files4training)):\n",
    "    files4training[i] = files4training[i][len(DATASET_PATH+ 'input/input/'):]\n",
    "    \n",
    "partition ={'train': files4training}\n",
    "\n",
    "# Train model on dataset\n",
    "model = get_unet(220,220,21,3,0.0001,'binary_crossentropy', 'Adam', 'accuracy', 32, 1254)\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'],**params)\n",
    "\n",
    "callbacks = []\n",
    "checkpoint = ModelCheckpoint('/home/esgomezm/Projects/3D-PROTUCEL/Code/binary001/checkpoints/2dUnet_weights_01_{epoch:05d}.hdf5', monitor='loss',\n",
    "                             save_weights_only=False, mode='auto', period=20)\n",
    "checkpoint.set_model(model)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./binary001/logs', batch_size=1, write_graph=True, write_grads=True, \n",
    "                          write_images=False, update_freq=10)\n",
    "tensorboard.set_model(model)\n",
    "callbacks.append(tensorboard)\n",
    "\n",
    "reducelearning = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=50, verbose=2, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "reducelearning.set_model(model)\n",
    "callbacks.append(reducelearning)\n",
    "\n",
    "model.load_weights(\"./binary001/checkpoints/2dUnet_weights_00160.hdf5\")\n",
    "\n",
    "model.fit_generator(generator=training_generator, epochs=(1000-160), callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIEmWzai5CKo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, BaseLogger, Callback, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "# from my_classes import DataGenerator\n",
    "\n",
    "DATASET_PATH = ('/home/esgomezm/Projects/3D-PROTUCEL/data/training/')\n",
    "params = {'dataset_path': DATASET_PATH,\n",
    "          'rotation_range' : 30,\n",
    "          'width_shift_range': 0.2,\n",
    "          'height_shift_range': 0.2,\n",
    "          'shear_range': 0.2,\n",
    "          'zoom_range': 0.2,\n",
    "          'horizontal_flip': True,\n",
    "          'fill_mode': 'reflect',\n",
    "          'patch_batch': 5}\n",
    "import glob\n",
    "files4training = glob.glob(DATASET_PATH + 'input/input/*.tif')\n",
    "for i in range(len(files4training)):\n",
    "    files4training[i] = files4training[i][len(DATASET_PATH+ 'input/input/'):]\n",
    "    \n",
    "partition ={'train': files4training}\n",
    "model = load_model(\"./binary001/checkpoints/2dUnet_weights_01_00060.hdf5\")\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'],**params)\n",
    "\n",
    "callbacks = []\n",
    "checkpoint = ModelCheckpoint('/home/esgomezm/Projects/3D-PROTUCEL/Code/binary001/checkpoints/2dUnet_weights_02_{epoch:05d}.hdf5', monitor='loss',\n",
    "                             save_weights_only=False, mode='auto', period=20)\n",
    "checkpoint.set_model(model)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./binary001/logs', batch_size=1, write_graph=True, write_grads=True, \n",
    "                          write_images=False, update_freq=10)\n",
    "tensorboard.set_model(model)\n",
    "callbacks.append(tensorboard)\n",
    "\n",
    "reducelearning = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=50, verbose=2, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "reducelearning.set_model(model)\n",
    "callbacks.append(reducelearning)\n",
    "\n",
    "model.fit_generator(generator=training_generator, epochs=(1000-160-60), callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGrjkL3z5CKu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "self = DataGenerator(partition['train'],**params)\n",
    "\n",
    "data = self.__getitem__(2)\n",
    "self.dataset_path = DATASET_PATH\n",
    "ID = 'video_10_000002.tif'\n",
    "aux_x = sitk.ReadImage(self.dataset_path + '/input/input/'+ ID)\n",
    "aux_x = sitk.GetArrayFromImage(aux_x)\n",
    "aux_x = np.transpose(aux_x,[0,2,1]) # x dimensions (z,x,y)\n",
    "aux_x = aux_x.reshape((aux_x.shape[0], aux_x.shape[1],aux_x.shape[2],1))\n",
    "\n",
    "aux_y = sitk.ReadImage(self.dataset_path + '/output/output/'+ ID)\n",
    "aux_y = sitk.GetArrayFromImage(aux_y)\n",
    "aux_y = np.transpose(aux_y,[1,0]) # y dimensions (x,y)\n",
    "aux_y = aux_y.reshape((1, aux_y.shape[0],aux_y.shape[1], 1))   \n",
    "\n",
    "# plt.imshow(aux_x[10,:,:,0])\n",
    "# plt.show()\n",
    "# plt.imshow(aux_y[0,:,:,0])\n",
    "# plt.show()\n",
    "seed = np.random.randint(100000)\n",
    "random_crop_size_input = (220,220)\n",
    "random_crop_size_output = (208,208)\n",
    "c = 0\n",
    "input_channel, output_channel = random_crop(aux_x[:,:,:,c], aux_y[:,:,:,c], random_crop_size_input, random_crop_size_output, sync_seed=seed)\n",
    "input_channel = (input_channel-np.min(input_channel))/(np.max(input_channel)-np.min(input_channel))\n",
    "\n",
    "plt.imshow(input_channel[10,:,:])\n",
    "plt.show()\n",
    "plt.imshow(output_channel[0,:,:])\n",
    "plt.show()\n",
    "\n",
    "input_channel_i = np.transpose(input_channel,[1,2,0])\n",
    "input_channel_i = input_channel_i.reshape((1, input_channel_i.shape[0], input_channel_i.shape[1], input_channel_i.shape[2]))\n",
    "p = model.predict(input_channel_i)\n",
    "plt.imshow(p[0,:,:,0])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BvvBoPrQYq9A"
   ],
   "name": "data_generators.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
